<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Text Mining Project</title>
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<link rel="icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div id="mobile-menu-open" class="shadow-large">
        <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
        <div id="mobile-menu-close">
            <span>Close</span> <i class="fa fa-times" aria-hidden="true"></i>
        </div>
        <ul id="menu" class="shadow">
            <li>
                <a href="#introduction">Introduction</a>
            </li>
            <li>
                <a href="#data">Data</a>
            </li>
            <li>
                <a href="#cluster">Clustering</a>
            </li>
            <li>
                <a href="#arm">ARM</a>
            </li>
            <li>
                <a href="#lda">LDA</a>
            </li>
            <li>
                <a href="#nb">Naive Bayes</a>
            </li>
            <li>
                <a href="#dt">Decision Tree</a>
            </li>
            <li>
                <a href="#svm">SVM</a>
            </li>
            <li>
                <a href="#nn">Neural Nets</a>
            </li>
            <li>
                <a href="#conclusion">Conclusion</a>
            </li>


        </ul>
    </header>
    <!-- End header -->

    <div id="lead">
        <div id="lead-content">
            <h1>TikTok Sentiment Analysis: App Store Reviews and Tweets</h1>
            <h2>Xingyu(Tim) Chen</h2>
            <a href="https://www.youtube.com/@xingyuchen" class="btn-rounded-white">Project Presentation</a>
        </div>
        <!-- End #lead-content -->

        <div id="lead-overlay"></div>

        <div id="lead-down">
            <span>
                <i class="fa fa-chevron-down" aria-hidden="true"></i>
            </span>
        </div>
        <!-- End #lead-down -->
    </div>
    <!-- End #lead -->

    <div id="introduction">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Introduction</h2>
                </div>
                <div class="col-md-8">
                    <p>
                        The landscape of social media is ever-changing, especially among teens who often are on the leading edge of this space. A new Pwe Research Center survey of American teenagers ages 13 to 17 finds TikTok has rocketed in popularity since its North American debut on September 2016 and now is a top social media platform for teens among the platforms covered in this survey.
                    </p>
                    <p></p>
                    <p>
                        YouTube tops the 2022 teen online landscape among the platforms covered in the Center’s new survey, as it is used by 95% of teens. It has over 3 billion downloads, and over 1 billion active users each month. TikTok is next on the list of platforms that were asked about in this survey (67%), followed by Instagram and Snapchat, which are both used by about six-in-ten teens. After those platforms come to Facebook with 32% and smaller shares who use Twitter, Twitch, WhatsApp, Reddit and Tumblr.
                    </p>
                    <p></p>
                    <p>
                        TikTok has faced several controversies since its launch. One of the most notable controversies is from 'Shorts'. A large body of research has shown that short videos are addictive because the app gets to know your preferences, so it delivers videos that match your taste. The shorts can decrease attention span from an overabundance of information through just scrolling. In the biology field, shorts can cause dopamine problems through gain lots of joy in a certain period.
                    </p>
                    <p>
                        On March 23, 2023, TikTok CEO Shou Zi Chew testifies in front of Congress in a five-hour hearing. Chew touted TikTok's commitment to security and free speech. Congress grilled him on data privacy and TikTok's ties to China. Since then, the #keeptiktok and #bantiktok has been widely used in social media: YouTube, Instagram, Twitter, etc.
                    </p>
                    <p>
                        Thus, this paper is focus on doing some text mining analysis on Tiktok: identify key insights related to the app, and key problems/issues people have raised; perform sentiment analysis of the reviews and find what people are talking about; perform topic modeling to identify key topics mentioned in the review over time; generate visualizations of different worlds/n-grams/topics extracted from the reviews.
                    </p>
                    <img src="images/pic1.png" alt="pic1">
                </div>
            </div>
        </div>
    </div>


    <div id="data">
        <div class="container">
        <div class="row">
            <div class="col-md-4">
                <h2 class="heading">Data</h2>
                <p></p>
                <a href = "https://github.com/XingyuChenTim/project-port/blob/master/newsapigather.ipynb">LINK TO CODE</a>
                <p></p>
                <a href = "https://github.com/XingyuChenTim/project-port/blob/master/NewHeadlines_2_5.csv">LINK TO DATA</a>
            </div>
            <div class="col-md-8">
                <p>
                    Two APIs have been used to gather data, one is newsapi, another is Google Search API.
                    Four labels: tiktokgood, tiktokbad, tiktokmentalhealth, tiktokjoy.
                    Here is the image of raw data:
                </p>
                <img src="images/pic2.png" alt="pic1">
                <img src="images/pic3.png" alt="pic1">
            <p></p>
            <p>
                The PorterStemmer has been used for Stemming and WordNet from NLTK for Lemmatization.
                Configured CountVectorizer with maximum feature 50 and TfidVectorizor with maximum feature 1000
                to clean the data. Here is the image of cleaned data:
            </p>
                <p></p>
                <img src="images/pic4.png" alt="pic1">
                <img src="images/pic5.png" alt="pic1">
            </div>
        </div>
        </div>
    </div>

    <div id="cluster">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Cluster</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/newsapigather.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/NewHeadlines_2_5.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p>
                            Clustering is a way to group a set of data points in a way that similar data points
                            are grouped together. Therefore, clustering algorithms look for similarities or dissimilarities
                            among data points so it can be used to discover groups, categories, and/or similarities within a dataset.
                            Clustering is an unsupervised learning method so there is no label associated
                            with data points. The algorithm tries to find the underlying structure of the data.
                    </p>
                    <p>
                        There are different approaches and algorithms to perform clustering tasks which can be divided into three sub-categories:
                    </p>
                    <ul>
                        <li> Partition-based clustering: E.g. k-means, k-median </li>
                        <li> Hierarchical clustering: E.g. Agglomeration, Divisive </li>
                        <li> Density-based clustering: E.g. DBSCAN </li>
                    </ul>
                    <p>
                        For current progress, the project focus on k-means for partition-based clustering in Python and
                        Hierarchical clustering in R.
                    </p>
                    <h3>Data prep:</h3>
                    <p>
                    </p>
                    <img src="images/pic6.png" alt="pic1">
                    <p></p>
                    <h3>Result:</h3>
                    <p>
                        A silhouette plot is a graphical tool we use to evaluate the quality of clusters.
                        The silhouette values show the degree of cohesion and separation of the clusters.
                        Silhouette values measure the relation between cluster cohesion and cluster separation.
                        Thus, the mean of the silhouette values represents the balance of the overall cohesion
                        and separation in all the clusters.
                    </p>
                    <ul>
                        <li> If silhouette values > 0.70, the structure of the clusters is strong. </li>
                        <li> If silhouette values is between 0.51 and 0.70 the structure is reasonable. </li>
                        <li> Lower values indicate poor structure.</li>
                    </ul>
                    <p>
                        K-means clustering is a simplest and popular unsupervised machine learning algorithms.
                        We can evaluate the algorithm by two ways such as elbow technique and silhouette technique.
                        We saw differences between them above.
                        The silhouette technique gives us more precise score and number of k for k-means algorithm.
                        However, we can also use elbow technique for quick response and intuition.
                        each object is assigned to its own cluster and then the algorithm proceeds iteratively,
                        at each stage joining the two most similar clusters,
                        continuing until there is just a single cluster.
                        At each stage distances between clusters are recomputed by the Lance--Williams dissimilarity
                        update formula according to the particular clustering method being used.
                    </p>
                    <p></p>
                    <img src="images/pic9.png" alt="pic1">
                    <img src="images/pic10.png" alt="pic1">
                    <img src="images/pic11.png" alt="pic1">
                </div>
            </div>
        </div>
    </div>

    <div id="arm">
        <div class="container">
        <div class="row">
            <div class="col-md-4">
                <h2 class="heading">ARM</h2>
                <p></p>
                <a href = "https://github.com/XingyuChenTim/project-port/blob/master/newsapigather.ipynb">LINK TO CODE(R)</a>
                <p></p>
                <a href = "https://github.com/XingyuChenTim/project-port/blob/master/NewHeadlines_2_5.csv">LINK TO DATA</a>
            </div>
            <div class="col-md-8">
                <h3>Overview:</h3>
                <p>
                    ARM(Association Rule Mining): Unsupervised/discovery: no labels, evaluate transactions for correlations/associations.

                </p>
                <h3>Data prep:</h3>
                <img src="images/pic15.png" alt="pic1">
                <p></p>
                <h3>Result:</h3>
                <p>

                </p>
                <ul>
                    <li> Support: [How often items in A and items in B occur together relative to all transactions.] </li>
                    <li> Confidence: [How often items in A and items in B occur together – relative to transactions that contain A] </li>
                    <li> Lift: Lift(A,B) > 1, A and B are positively correlated</li>
                </ul>
                <p></p>
                <img src="images/pic16.png" alt="pic1">
                <p>
                    Include at least 2 visualizations (networks) that show the associations you found.
                </p>
                <img src="images/pic14.png" alt="pic1">
                <img src="images/pic13.png" alt="pic1">
                <img src="images/pic12.png" alt="pic1">
            </div>
        </div>
        </div>
    </div>

    <div id="lda">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">LDA</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/newsapigather.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/NewHeadlines_2_5.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p>
                        Topic modeling
                    </p>
                    <h3>Data prep:</h3>
                    <p>
                    </p>
                    <img src="images/pic8.png" alt="pic1">
                    <p></p>
                    <h3>Result:</h3>
                    <p>
                    </p>
                    <p></p>
                </div>
            </div>
        </div>
    </div>

    <div id="sth">
        <div class="container">
            <div class="row">
                <div id="includedContent1"></div>
                <div id="includedContent2"></div>
            </div>
        </div>
    </div>


    <div id="nb">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Naive Bayes</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/dt_nb_svm.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/df_count.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p>
                        For supervised model approach, each content has been kept length less than 50 words. And to keep the decision tree readable, the max_features have been kept as 50. And the labels have been transfer into categories values. The numerical values from 1 to 5 transform into "very bad", "bad", "average", "good", and "very good".
                    </p>
                    <h3>Data Prep:</h3>
                    <p>
                    STEPS:  
                    <p>1) Managing Missing Values    
                    <p>2) Managing Incorrect (or incorrectly formatted) Values    
                    <p>3) Dealing with duplicates    
                    <p>4) Managing outliers     
                    </p>
                    <img src="images/pic17.png" alt="pic1", width="600px">
                    <p>
                    </p>
                    <h3>Result & Conclusion:</h3>
                    <p>
                        Naïve Bayes is a supervised method that uses labeled data to train a probability-based prediction model. This model can then be used to predict/classify data vectors for which the label is not known. 
                        <p>    The Naive Bayes algorithm is based on Bayes' theorem, which states that the probability of a hypothesis given the observed evidence is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis. In text classification, the hypothesis is the class label (e.g., positive or negative sentiment) and the evidence is the words in the text.
                            <p>   Naive Bayes is called "naive" because it makes the assumption that the presence or absence of a word in a document is independent of the presence or absence of any other word in the document, which is often not true in practice. Despite this assumption, Naive Bayes often performs well in text classification tasks and is computationally efficient, making it a popular choice for many applications.
                    <p> From the confusion matrix below, the Naive Bayes perform relatively well for each categories. </p>
                            </p>
                    <img src="images/pic20.png" alt="pic1">
                    <p></p>
                </div>
            </div>
        </div>
    </div>

    <div id="dt">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Decision Tree</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/dt_nb_svm.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/df_count.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p> In other words, directional tree: each leaf node is assigned a class label. It is easier to construct compared to other supervised models and easy to interpret for small-sized trees structure.</p>
                    <p>Robust for missing values but have training data pruning for overfitting issue. And also hard to define stopping criteria and chooseing and ordering of splitting attributes</p>
                    <p>A decision tree is a type of supervised learning algorithm that recursively partitions the data into subsets based on the values of one or more features, with the goal of maximizing the purity or homogeneity of each subset with respect to the target variable.</p>
                    <h3>Data Prep:</h3>
                    <p>
                        There are 354,159 samples in the dataset. However the data is not evenly distributed, which make model has biase on certain categories. The top frequency categories values is "very good" and the less frequency categories values is "bad". Thus downsampling each categories to 5000 so make each sample from categories value evenly. 
                        We have 5 categories values thus we totaly have 25,000 rows of samples. 
                    </p>
                    <img src="images/pic18.png" alt="pic1", width="800px">
                    </p>
                    </p>
                    <h3>Result & Conclusion:</h3>
                    <p>Decision trees can be prone to overfitting, particularly when the tree is deep and complex. This can lead to poor generalization performance on new data. Thus, the maximum leaft nodes set to 5 in order to interpret and visualize, which can help in understanding the factors that are driving the predictions </p>
                    <p> Decision Tree 1: entropy criterion + best splitter. </p>
                <img src="images/pic22.png" alt="pic1">
                <p> Decision Tree 2: entropy criterion + random splitter </p>
                <img src="images/pic23.png" alt="pic1">
                <p>Decision Tree 3: gini criterion + best splitter </p>
                <img src="images/pic24.png" alt="pic1">
                <p>Due to limited leaf set up, all the three decison perform bad on predictions. However, we can find top 5 key features for spliting the decision trees. Top 4 features for split trees: good, love, great, nice</p>
                <img src="images/pic21.png" alt="pic1">
                </div>
            </div>
        </div>
    </div>

    <div id="svm">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">SVM</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/dt_nb_svm.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/df_count.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p> SVM is a type of supervised learning algorithm that seeks to find the hyperplane that best separates the data points of different classes in a high-dimensional space, using different Kernels to change the margin. 
                    <p>In the context of text classification, SVM can be used to classify documents into different categories based on the presence or absence of certain words or phrases in the text. 
                    <p>The SVM algorithm tries to find the hyperplane that maximizes the margin between the data points of different classes. The margin is the distance between the hyperplane and the closest data points of each class. SVM tries to minimize the misclassification error while maximizing the margin.</p>
                    <p>SVM has several advantages in text mining and NLP applications. It can handle both linearly and non-linearly separable data and can work well even in high-dimensional spaces. SVM is also robust to overfitting and can generalize well to new data.</p> 
                    <h3>Data Prep:</h3>
                    <p>
                        After downsampling, we have 25,000 rows before we split into training and testing dataset, the ratio is 20% verse 80%.  
                    </p>
                    <img src="images/pic19.png" alt="pic1">
                    <p>
                    <h3>Result & Conclusion:</h3>
                    <p>One disadvantage of SVM is that it can be computationally expensive and time-consuming to train on large datasets with a large number of features. 
                        <p>Another disadvantage is that the choice of the kernel function used in SVM can have a significant impact on the performance of the model, and selecting the right kernel function for a given dataset can be challenging.</p>
                    <p> There are three kernels been used for SVM model: LinearSVC, RBF, and POLY. The POLY perfom better compared to Linear and RBF kernel. 
                        <p>LinearSVC</p>
                    <img src="images/pic25.png" alt="pic1">
                    <p>RBF</p>
                    <img src="images/pic26.png" alt="pic1">
                    <p>POLY</p>
                    <img src="images/pic27.png" alt="pic1">
                    <p>SVM is perform relately well compared to Decision Tree and Naive Bayes with similary time and memory consumption. SVM can be used in combination with other techniques such as feature selection and ensemble methods to improve its performance on text classification tasks.</p>
                    <p>However, SVM is hard to interpret because it is "blackbox" model and it is hard to know which features has been significant contribute to the prediction or categories result.</p>
                </div>
            </div>
        </div>
    </div>

    <div id="nn">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Neural Nets</h2>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/nn_project.ipynb">LINK TO CODE</a>
                    <p></p>
                    <a href = "https://github.com/XingyuChenTim/project-port/blob/master/df_nn.csv">LINK TO DATA</a>
                </div>
                <div class="col-md-8">
                    <h3>Overview:</h3>
                    <p>
                        Neural networks are a powerful tool for text mining tasks such as text classification, sentiment analysis, and language modeling. They have been widely used in natural language processing (NLP) due to their ability to learn complex patterns and relationships in large amounts of text data.
                    </p>
                    <p>There are three typical neural networks been used in this section: LSTM (RNN), CNN, and BERT (Transformer)</p>
                    <h3>Data prep:</h3>
                    <p>
                        Due to complex of neural nets, the sample dataset only contain two labeled, negative for '0' and positive for '1'. Max_features has been set up as 62.  
                    </p>
                    <img src="images/pic30.png" alt="pic1", width="800px">
                    <p></p>
                    <h3>Result & Conclusion:</h3>
                    <p>  LSTM (RNN)</p>
                    <p>Recurrent neural networks (RNNs) are designed to handle sequential data, such as text, by maintaining a memory of previous inputs. RNNs have been shown to be particularly effective at modeling long-term dependencies in the input text, making them well-suited for tasks that involve longer text sequences, such as language modeling or text generation. However, they can be prone to the vanishing gradient problem, which can make it difficult for the RNN to learn long-term dependencies.
                        <p>    To address this problem, long short-term memory (LSTM) have been proposed, which use gating mechanisms to selectively update the hidden state and prevent the vanishing gradient problem. During training, the LSTM network learns to update its hidden state based on the input word embeddings and the previous hidden state. The output of the last LSTM cell is then fed into a fully connected layer, which performs the final classification or prediction</p>
                    <img src="images/pic28.png" alt="pic1">
                    <p>  CNN </p>
                    <p>Convolutional neural networks (CNNs) using word embeddings as input, which is a sequence of words. In a convolutional layer, the CNN applies a set of filters to the input sequence, with each filter sliding over the sequence to detect patterns at different positions. The output of the convolutional layer is a set of feature maps, where each feature map represents the activation of a particular filter across the input sequence. </p>
                    <img src="images/pic29.png" alt="pic1">
                    <p>  BERT (Transformer) </p>
                    <p> DistilBERT is a distilled and smaller version of the BERT (Bidirectional Encoder Representations from Transformers) model that has been designed to be faster and more memory-efficient while still maintaining high performance on various natural language processing (NLP) tasks, including text mining.</p>
                    <p> Like BERT, DistilBERT is a transformer-based neural network architecture that uses self-attention mechanisms to capture the semantic and syntactic relationships between words in a sentence. However, DistilBERT achieves this with fewer layers and fewer parameters, resulting in a smaller model that can be trained and used more efficiently.</p>
                    <img src="images/pic31.png" alt="pic1">
                    <img src="images/pic32.png" alt="pic1">
                    <p>
                    <p>We can see that the BERT perform better than CNN and LSTM(RNN) but spending more time. BERT has 86% accuracy meanwhile LSTM has 72% and CNN has 78%. </p>
                </div>
            </div>
        </div>
    </div>

    <div id="conclusion">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="heading">Conclusion</h2>
                </div>
                <div class="col-md-8">
                    <p>
                        TBD
                    </p>
                    <p></p>
                </div>
            </div>
        </div>
    </div>


    <footer class="background-alt">
        <div class="container">
            <div class="row">
                <div class="col-sm-5 copyright">
                    <p>
                        Copyright &copy; <span id="current-year">2023</span> XINGYU(TIM) CHEN
                    </p>
                </div>
                <div class="col-sm-2 top">
                    <span id="to-top">
                        <i class="fa fa-chevron-up" aria-hidden="true"></i>
                    </span>
                </div>
                <div class="col-sm-5 social">
                    <ul>
                        <li>
                            <a href="https://github.com/XingyuChenTim" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://www.linkedin.com/in/xingyu-tim-chen/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
    <script src="jquery.js"></script>
    <script>
        $(function(){
            $("#includedContent1").load("lda1.html");
            $("#includedContent2").load("lda2.html");
        });
    </script>
</body>

</html>
